# (c) 2017 DataNexus Inc.  All Rights Reserved
---
# first, load the local variables file (if one was defined); this will initialize
# the variables used in our playbook (and override any values in the 'vars/spark.yml'
# file with redefined values from the 'local_vars_file', if any)
- name: Load local variables file (if defined)
  include_vars:
    file: "{{local_vars_file}}"
  when: not (local_vars_file is undefined or local_vars_file is none or local_vars_file | trim == '')
# then, restart the network (unless the skip_network_restart was set)
# and gather some facts about our input node(s)
- name: Ensure the network interfaces are up on our input node(s)
  service:
    name: network
    state: restarted
  become: true
  when: not (skip_network_restart is defined or skip_network_restart)
- name: Gather facts from the input node(s)
  setup:
# next, we obtain the interface names for our data_iface
# and api_iface (provided an interface description was provided for each)
- include_role:
    name: get-iface-names
  vars:
    iface_descriptions: "{{iface_description_array}}"
  when: not (iface_description_array is undefined or iface_description_array == [])
# and now that we know we have our data_iface identified, we can construct
# the list of zk_nodes (the data_iface IP addresses of our zookeeper_nodes)
- set_fact:
    zk_nodes: "{{(zookeeper_nodes | default([])) | map('extract', hostvars, [('ansible_' + data_iface), 'ipv4', 'address']) | list}}"
# if we're provisioning a RHEL machine, then we need to ensure that
# it's subscribed before we can install anything (if it hasn't been
# registered already, of course, if that's the case then we can skip
# this step)
- block:
  - redhat_subscription:
      state: present
      username: "{{rhel_username}}"
      password: "{{rhel_password}}"
      consumer_id: "{{rhel_consumer_id}}"
    become: true
    when: rhel_username is defined and rhel_password is defined and rhel_consumer_id is defined
  when: ansible_distribution == 'RedHat'
# Now that we have all of the facts we need, we can run the roles that are used to
# deploy and configure Spark
- include_role:
    name: get-iface-addr
  vars:
    iface_name: "{{data_iface}}"
    as_fact: "data_addr"
- include_role:
    name: get-iface-addr
  vars:
    iface_name: "{{api_iface}}"
    as_fact: "api_addr"
- include_role:
    name: setup-web-proxy
- include_role:
    name: add-local-repository
  vars:
    yum_repository: "{{yum_repo_url}}"
  when: yum_repo_url is defined
- include_role:
    name: install-packages
  vars:
    package_list: "{{combined_package_list}}"
- include_role:
    name: get-java-home
- include_role:
    name: dn-spark